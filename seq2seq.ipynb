{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandhc6/Assignment-3/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhDxe46ViCCn"
      },
      "outputs": [],
      "source": [
        "# Required packages\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import backend\n",
        "from random import randrange \n",
        "from tensorflow import keras\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zRhhFwXid4u",
        "outputId": "6090a465-70d4-4ab1-a489-f17c3e0ed720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-07 12:42:49--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.45.112, 172.217.15.80, 172.253.62.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.45.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   161MB/s    in 13s     \n",
            "\n",
            "2022-05-07 12:43:02 (142 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading dataset\n",
        "\n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf 'dakshina_dataset_v1.0.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQdGUuUTDj1m"
      },
      "outputs": [],
      "source": [
        " # Paths of train and valid datasets.\n",
        "train_data_path = \"dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
        "val_data_path = \"dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
        "\n",
        "# Saving the files in list\n",
        "with open(train_data_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    train_data_lines = file.read().split(\"\\n\")\n",
        "\n",
        "with open(val_data_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    val_data_lines = file.read().split(\"\\n\")\n",
        "\n",
        "# popping the empty character of the lists \n",
        "val_data_lines.pop()\n",
        "train_data_lines.pop()\n",
        "\n",
        "# Fixed parameter\n",
        "batch_size = 64 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6wy9xCNDc0u"
      },
      "outputs": [],
      "source": [
        "# embedding train data\n",
        "\n",
        "def embed_train_data(train_data_lines):\n",
        "\n",
        "    lenk = len(train_data_lines) - 1\n",
        "    train_input_data = []\n",
        "    train_target_data = []\n",
        "    input_data_characters = set()\n",
        "    target_data_characters = set()\n",
        "    \n",
        "    for line in train_data_lines[: lenk]:\n",
        "        target_data, input_data, _ = line.split(\"\\t\")\n",
        "\n",
        "        # We are using \"tab\" as the \"start sequence\" and \"\\n\" as \"end sequence\".\n",
        "        target_data = \"\\t\" + target_data + \"\\n\"\n",
        "        train_input_data.append(input_data)\n",
        "        train_target_data.append(target_data)\n",
        "\n",
        "        # Finding unique characters.\n",
        "        for ch in input_data:\n",
        "            if ch not in input_data_characters:\n",
        "                input_data_characters.add(ch)\n",
        "        for ch in target_data:\n",
        "            if ch not in target_data_characters:\n",
        "                target_data_characters.add(ch)\n",
        "\n",
        "    print(\"Number of samples:\", len(train_input_data))\n",
        "    # adding space \n",
        "    input_data_characters.add(\" \")\n",
        "    target_data_characters.add(\" \")\n",
        "\n",
        "    # sorting\n",
        "    input_data_characters = sorted(list(input_data_characters))\n",
        "    target_data_characters = sorted(list(target_data_characters))\n",
        "\n",
        "    # maximum length of the words\n",
        "    encoder_max_length = max([len(txt) for txt in train_input_data])\n",
        "    decoder_max_length = max([len(txt) for txt in train_target_data])\n",
        "\n",
        "    print(\"Max sequence length for inputs:\", encoder_max_length)\n",
        "    print(\"Max sequence length for outputs:\", decoder_max_length)\n",
        "\n",
        "    # number of input and target characters\n",
        "    num_encoder_tokens = len(input_data_characters)\n",
        "    num_decoder_tokens = len(target_data_characters)  \n",
        "    \n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "\n",
        "    # create an index\n",
        "    input_token_idx = dict([(char, i) for i, char in enumerate(input_data_characters)])\n",
        "    target_token_idx = dict([(char, i) for i, char in enumerate(target_data_characters)])\n",
        "   \n",
        "    # creating 0 array for encoder,decoder \n",
        "    encoder_input_data = np.zeros((len(train_input_data), encoder_max_length), dtype=\"float32\")\n",
        "\n",
        "    decoder_input_data = np.zeros((len(train_input_data), decoder_max_length), dtype=\"float32\")\n",
        "\n",
        "    decoder_target_data = np.zeros((len(train_input_data), decoder_max_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    # index of the character is encoded for all the sample whereas target data is one hot encoded.\n",
        "    for i, (input_data, target_data) in enumerate(zip(train_input_data, train_target_data)):\n",
        "        for t, char in enumerate(input_data):\n",
        "            encoder_input_data[i, t] = input_token_idx[char]\n",
        "        \n",
        "        encoder_input_data[i, t + 1:] = input_token_idx[\" \"]\n",
        "        \n",
        "        # decoder data\n",
        "        for t, char in enumerate(target_data):\n",
        "            # decoder_target_data is one timestep ahead of decoder_input_data\n",
        "            decoder_input_data[i, t] = target_token_idx[char]\n",
        "\n",
        "            if t > 0:\n",
        "                # excluding the start character since decoder target data is one timestep ahead.\n",
        "                decoder_target_data[i, t - 1, target_token_idx[char]] = 1.0\n",
        "        # append the remaining positions with empty space\n",
        "       \n",
        "        decoder_input_data[i, t + 1:] = target_token_idx[\" \"]\n",
        "        decoder_target_data[i, t:, target_token_idx[\" \"]] = 1.0\n",
        "\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data,num_encoder_tokens,num_decoder_tokens,input_token_idx,target_token_idx,encoder_max_length,decoder_max_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N80Xht1DZvW"
      },
      "outputs": [],
      "source": [
        "# embedding validation data\n",
        "\n",
        "def embed_val_data(val_data_lines,num_decoder_tokens,input_token_idx,target_token_idx):\n",
        "    val_input_data = []\n",
        "    val_target_data = []\n",
        "    lenk = len(val_data_lines) - 1\n",
        "\n",
        "    for line in val_data_lines[: lenk]:\n",
        "        target_data, input_data, _ = line.split(\"\\t\")\n",
        "        \n",
        "        # We use \"tab\" as the \"start sequence\" character and \"\\n\" as \"end sequence\" character.\n",
        "        target_data = \"\\t\" + target_data + \"\\n\"\n",
        "        val_input_data.append(input_data)\n",
        "        val_target_data.append(target_data)\n",
        "\n",
        "    val_encoder_max_length = max([len(txt) for txt in val_input_data])\n",
        "    val_decoder_max_length = max([len(txt) for txt in val_target_data])\n",
        "\n",
        "    val_encoder_input_data = np.zeros((len(val_input_data), val_encoder_max_length), dtype=\"float32\")\n",
        "    val_decoder_input_data = np.zeros((len(val_input_data), val_decoder_max_length), dtype=\"float32\")\n",
        "    val_decoder_target_data = np.zeros((len(val_input_data), val_decoder_max_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, (input_data, target_data) in enumerate(zip(val_input_data, val_target_data)):\n",
        "        for t, ch in enumerate(input_data):\n",
        "            val_encoder_input_data[i, t] = input_token_idx[ch]\n",
        "        val_encoder_input_data[i, t + 1:] = input_token_idx[\" \"]\n",
        "        \n",
        "        for t, ch in enumerate(target_data):\n",
        "            # decoder_target_data is one timestep ahead of decoder_input_data\n",
        "            val_decoder_input_data[i, t] = target_token_idx[ch]\n",
        "            if t > 0:\n",
        "                # excluding the start character since decoder target data is one timestep ahead.\n",
        "                val_decoder_target_data[i, t - 1, target_token_idx[ch]] = 1.0\n",
        "       \n",
        "        val_decoder_input_data[i, t + 1:] = target_token_idx[\" \"]\n",
        "        val_decoder_target_data[i, t:, target_token_idx[\" \"]] = 1.0\n",
        "\n",
        "    return val_encoder_input_data,val_decoder_input_data,val_decoder_target_data,target_token_idx,val_target_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SphcvHgrD6D2",
        "outputId": "b133ffc5-b2ee-4fc3-bd19-4546902933f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 68217\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n"
          ]
        }
      ],
      "source": [
        "# Embedding data\n",
        "encoder_input_data,decoder_input_data,decoder_target_data,num_encoder_tokens,num_decoder_tokens,input_token_idx,target_token_idx,encoder_max_length,decoder_max_length = embed_train_data(train_data_lines)\n",
        "\n",
        "val_encoder_input_data,val_decoder_input_data,val_decoder_target_data,target_token_idx,val_target_data = embed_val_data(val_data_lines,num_decoder_tokens,input_token_idx,target_token_idx)\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_idx.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_idx.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLqfVhxah7U6"
      },
      "outputs": [],
      "source": [
        "# RNN model\n",
        "\n",
        "def seq2seq(embedding_size, n_encoder_tokens, n_decoder_tokens, n_encoder_layers,n_decoder_layers, latent_dimension, cell_type,\n",
        "            target_token_idx, decoder_max_length, reverse_target_char_index,dropout,encoder_input_data, decoder_input_data,\n",
        "            decoder_target_data,batch_size,epochs):\n",
        "  \n",
        "  encoder_inputs = keras.Input(shape=(None,), name='encoder_input')\n",
        "\n",
        "  encoder = None\n",
        "  encoder_outputs = None\n",
        "  state_h = None\n",
        "  state_c = None\n",
        "  e_layer= n_encoder_layers\n",
        "  \n",
        "  # RNN\n",
        "\n",
        "  if cell_type==\"RNN\":\n",
        "    embed = tf.keras.layers.Embedding(input_dim=n_encoder_tokens, output_dim=embedding_size,name='encoder_embedding')(encoder_inputs)\n",
        "    encoder = keras.layers.SimpleRNN(latent_dimension, return_state=True, return_sequences=True,name='encoder_hidden_1', dropout=dropout)\n",
        "    print(\"Embed done\")\n",
        "    encoder_outputs, state_h = encoder(embed)\n",
        "    \n",
        "    for i in range(2,e_layer+1):\n",
        "      layer_name = ('encoder_hidden_%d') % i\n",
        "      print(\"Starting 2nd\")\n",
        "      encoder = keras.layers.SimpleRNN(latent_dimension, return_state=True, return_sequences=True,name=layer_name, dropout=dropout)\n",
        "      print(\"Ending 2nd\")\n",
        "\n",
        "      encoder_outputs, state_h = encoder(encoder_outputs, initial_state=[state_h])\n",
        "\n",
        "    encoder_states = None\n",
        "    encoder_states = [state_h]\n",
        "    decoder_inputs = keras.Input(shape=(None,), name='decoder_input')\n",
        "    embed_dec = tf.keras.layers.Embedding(n_decoder_tokens, embedding_size, name='decoder_embedding')(decoder_inputs)\n",
        "    \n",
        "    # number of decoder layers\n",
        "    d_layer = n_decoder_layers\n",
        "    decoder = None\n",
        "    decoder = keras.layers.SimpleRNN(latent_dimension, return_sequences=True, return_state=True,name='decoder_hidden_1', dropout=dropout)\n",
        "    \n",
        "    # initial state of decoder is encoder's last state of last layer\n",
        "    decoder_outputs, _ = decoder(embed_dec, initial_state=encoder_states)\n",
        "    for i in range(2,d_layer+1):\n",
        "      layer_name = 'decoder_hidden_%d' % i\n",
        "      decoder = keras.layers.SimpleRNN(latent_dimension, return_sequences=True, return_state=True,name=layer_name, dropout=dropout)\n",
        "      decoder_outputs, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
        "    \n",
        "    decoder_dense = keras.layers.Dense(n_decoder_tokens, activation=\"softmax\", name='decoder_output')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    \n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=['accuracy'])                 \n",
        "\n",
        "    model.fit(\n",
        "          [encoder_input_data, decoder_input_data],\n",
        "          decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=WandbCallback()\n",
        "      )\n",
        "    \n",
        "    # Inference model\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_outputs, state_h_enc = model.get_layer('encoder_hidden_' + str(n_encoder_layers)).output\n",
        "    encoder_states = [state_h_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1] \n",
        "    decoder_outputs = model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "    decoder_states_inputs = []\n",
        "    decoder_states = []\n",
        "\n",
        "    for j in range(1, n_decoder_layers + 1):\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dimension,))\n",
        "        current_states_inputs = [decoder_state_input_h]\n",
        "        decoder = model.get_layer('decoder_hidden_' + str(j))\n",
        "        decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=current_states_inputs)\n",
        "        decoder_states += [state_h_dec]\n",
        "        decoder_states_inputs += current_states_inputs\n",
        "\n",
        "    decoder_dense = model.get_layer('decoder_output')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "  # GRU\n",
        "\n",
        "  elif cell_type==\"GRU\":\n",
        "    embed = tf.keras.layers.Embedding(input_dim=n_encoder_tokens, output_dim=embedding_size,name='encoder_embedding')(encoder_inputs)\n",
        "    encoder = keras.layers.GRU(latent_dimension, return_state=True, return_sequences=True,name='encoder_hidden_1', dropout=dropout)\n",
        "    encoder_outputs, state_h = encoder(embed)\n",
        "    \n",
        "    for i in range(2,e_layer+1):\n",
        "      layer_name = ('encoder_hidden_%d') % i\n",
        "      encoder = keras.layers.GRU(latent_dimension, return_state=True, return_sequences=True,name=layer_name, dropout=dropout)\n",
        "      encoder_outputs, state_h = encoder(encoder_outputs, initial_state=[state_h])\n",
        "    \n",
        "    encoder_states = None\n",
        "    encoder_states = [state_h]\n",
        "    decoder_inputs = keras.Input(shape=(None,), name='decoder_input')\n",
        "    embed_dec = tf.keras.layers.Embedding(n_decoder_tokens, embedding_size, name='decoder_embedding')(decoder_inputs)\n",
        "    \n",
        "    # number of decoder layers\n",
        "    d_layer = n_decoder_layers\n",
        "    decoder = None\n",
        "    decoder = keras.layers.GRU(latent_dimension, return_sequences=True, return_state=True,name='decoder_hidden_1', dropout=dropout)\n",
        "    \n",
        "    # initial state of decoder is encoder's last state of last layer\n",
        "    decoder_outputs, _ = decoder(embed_dec, initial_state=encoder_states)\n",
        "    for i in range(2,d_layer+1):\n",
        "      layer_name = 'decoder_hidden_%d' % i\n",
        "      decoder = keras.layers.GRU(latent_dimension, return_sequences=True, return_state=True,name=layer_name, dropout=dropout)\n",
        "      decoder_outputs, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
        "    \n",
        "    decoder_dense = keras.layers.Dense(n_decoder_tokens, activation=\"softmax\", name='decoder_output')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=['accuracy'])#, metrics=[my_metric]                 \n",
        "\n",
        "    model.fit(\n",
        "          [encoder_input_data, decoder_input_data],\n",
        "          decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=WandbCallback()\n",
        "      )\n",
        "    \n",
        "    # Inference Model\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_outputs, state_h_enc = model.get_layer('encoder_hidden_' + str(n_encoder_layers)).output\n",
        "    encoder_states = [state_h_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_outputs = model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "    decoder_states_inputs = []\n",
        "    decoder_states = []\n",
        "\n",
        "    for j in range(1, n_decoder_layers + 1):\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dimension,))\n",
        "        current_states_inputs = [decoder_state_input_h]\n",
        "        decoder = model.get_layer('decoder_hidden_' + str(j))\n",
        "        decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=current_states_inputs)\n",
        "        decoder_states += [state_h_dec]\n",
        "        decoder_states_inputs += current_states_inputs\n",
        "    \n",
        "    decoder_dense = model.get_layer('decoder_output')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "  # LSTM\n",
        "\n",
        "  elif cell_type==\"LSTM\":\n",
        "    embed = tf.keras.layers.Embedding(input_dim=n_encoder_tokens, output_dim=embedding_size,name='encoder_embedding')(encoder_inputs)\n",
        "    encoder = keras.layers.LSTM(latent_dimension, return_state=True, return_sequences=True,name='encoder_hidden_1', dropout=dropout)\n",
        "    encoder_outputs, state_h, state_c = encoder(embed)\n",
        "    \n",
        "    for i in range(2,e_layer+1):\n",
        "      layer_name = ('encoder_hidden_%d') % i\n",
        "      encoder = keras.layers.LSTM(latent_dimension, return_state=True, return_sequences=True,name=layer_name, dropout=dropout)\n",
        "      encoder_outputs, state_h, state_c = encoder(encoder_outputs, initial_state=[state_h,state_c])\n",
        "    \n",
        "    encoder_states = None\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None,), name='decoder_input')\n",
        "    embed_dec = tf.keras.layers.Embedding(n_decoder_tokens, embedding_size, name='decoder_embedding')(decoder_inputs)\n",
        "    \n",
        "    # number of decoder layers\n",
        "    d_layer = n_decoder_layers\n",
        "    decoder = None\n",
        "    decoder = keras.layers.LSTM(latent_dimension, return_sequences=True, return_state=True,name='decoder_hidden_1', dropout=dropout)\n",
        "    \n",
        "    # initial state of decoder is encoder's last state of last layer\n",
        "    decoder_outputs, _,_ = decoder(embed_dec, initial_state=encoder_states)\n",
        "    for i in range(2,d_layer+1):\n",
        "      layer_name = 'decoder_hidden_%d' % i\n",
        "      decoder = keras.layers.LSTM(latent_dimension, return_sequences=True, return_state=True,name=layer_name, dropout=dropout)\n",
        "      decoder_outputs, _,_ = decoder(decoder_outputs, initial_state=encoder_states)\n",
        "    \n",
        "    decoder_dense = keras.layers.Dense(n_decoder_tokens, activation=\"softmax\", name='decoder_output')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=['accuracy'])#, metrics=[my_metric]                 \n",
        "    \n",
        "    model.fit(\n",
        "          [encoder_input_data, decoder_input_data],\n",
        "          decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=WandbCallback()\n",
        "      )\n",
        "    \n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer('encoder_hidden_' + str(n_encoder_layers)).output\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_outputs = model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "    decoder_states_inputs = []\n",
        "    decoder_states = []\n",
        "\n",
        "    for j in range(1,n_decoder_layers + 1):\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dimension,))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dimension,))\n",
        "        current_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder = model.get_layer('decoder_hidden_' + str(j))\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=current_states_inputs)\n",
        "        decoder_states += [state_h_dec, state_c_dec]\n",
        "        decoder_states_inputs += current_states_inputs\n",
        "    \n",
        "    decoder_dense = model.get_layer('decoder_output')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlClTo3fqaOu"
      },
      "outputs": [],
      "source": [
        "def accuracy(val_encoder_input_data, val_target_data,n_decoder_layers,encoder_model,decoder_model, verbose=False):\n",
        "        correct_count = 0\n",
        "        total_count = 0\n",
        "        n_val_data=len(val_encoder_input_data)\n",
        "        for seq_idx in range(n_val_data):\n",
        "            # Taking one sequence \n",
        "            input_charseq = val_encoder_input_data[seq_idx: seq_idx + 1]\n",
        "\n",
        "            states_val = [encoder_model.predict(input_charseq)]*n_decoder_layers\n",
        "\n",
        "            empty_charseq = np.zeros((1, 1))\n",
        "            # adding first character of target sequence with the start character.\n",
        "            empty_charseq[0, 0] = target_token_idx[\"\\t\"]\n",
        "            target_charseq = empty_charseq\n",
        "\n",
        "    \n",
        "            stop_cond = False\n",
        "            decoded_sentence = \"\"\n",
        "            while not stop_cond:\n",
        "                if cell_type is not None and (cell_type.lower() == 'rnn' or cell_type.lower() == 'gru'):\n",
        "                    temp = decoder_model.predict([target_charseq] + [states_val])\n",
        "                    output_tokens, states_val = temp[0], temp[1:]\n",
        "                else:\n",
        "                    temp = decoder_model.predict([target_charseq] + states_val )\n",
        "                    output_tokens, states_val = temp[0], temp[1:]\n",
        "\n",
        "                # Sample a token\n",
        "                sampled_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "                sampled_character = reverse_target_char_index[sampled_token_idx]\n",
        "                decoded_sentence += sampled_character\n",
        "\n",
        "                if sampled_character == \"\\n\" or len(decoded_sentence) > decoder_max_length:\n",
        "                    stop_cond = True\n",
        "\n",
        "                # Updating the target sequence.\n",
        "                target_charseq = np.zeros((1, 1))\n",
        "                target_charseq[0, 0] = sampled_token_idx\n",
        "\n",
        "            if decoded_sentence.strip() == val_target_data[seq_idx].strip():\n",
        "                correct_count += 1\n",
        "\n",
        "            total_count += 1\n",
        "\n",
        "            if verbose:\n",
        "                print('Prediction ', decoded_sentence.strip(), ',Ground Truth ', val_target_data[seq_idx].strip())\n",
        "        \n",
        "        accuracy =correct_count * 100.0 / total_count\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSTR6NrSJ4cj",
        "outputId": "226ae644-1db0-441f-8409-e1b914c76920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 26.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=9150ddef3aa9d389342b2082229f6fe4563f9c5f249cdf6cef00edca40aabf02\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGScPi2aJoTr"
      },
      "outputs": [],
      "source": [
        "def fit():\n",
        "  config_defaults = {\n",
        "            \"cell_type\":'LSTM',\n",
        "            \"num_encoder_layers\":2,\n",
        "            \"num_decoder_layers\":3,\n",
        "            \"embedding_size\":256,\n",
        "            \"latent_dimension\":256,\n",
        "            \"dropout\":0.2,\n",
        "            \"epochs\":25\n",
        "        }\n",
        "  wandb.init(config=config_defaults)\n",
        "\n",
        "  config = wandb.config\n",
        "  \n",
        "  cell_type=config.cell_type\n",
        "  n_encoder_layers=config.num_encoder_layers\n",
        "  n_decoder_layers=config.num_decoder_layers\n",
        "  embedding_size=config.embedding_size\n",
        "  latent_dimension=config.latent_dimension\n",
        "  dropout=config.dropout\n",
        "  epochs=config.epochs\n",
        "\n",
        "  run_name = \"cell_type_{}_nel_{}_ndl_{}_drop_{}_emd_{}_ld_{}\".format(cell_type, n_encoder_layers, n_decoder_layers, dropout, embedding_size, latent_dimension )\n",
        "  \n",
        "  encoder_model, decoder_model=seq2seq(embedding_size, num_encoder_tokens,num_decoder_tokens,n_encoder_layers, n_decoder_layers,latent_dimension,\n",
        "                cell_type, target_token_idx, decoder_max_length,reverse_target_char_index, dropout ,encoder_input_data, decoder_input_data,\n",
        "                decoder_target_data,batch_size,epochs)\n",
        "  \n",
        "  val_accuracy=accuracy(val_encoder_input_data, val_target_data,n_decoder_layers,encoder_model,decoder_model)\n",
        "  print(\"Validation Accuracy:\", val_accuracy)\n",
        "  wandb.log({'val_accuracy': val_accuracy})\n",
        "  wandb.run.name = run_name\n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "key = 012003eeed065050f00940856a48fb3f54ab471b"
      ],
      "metadata": {
        "id": "WfZYwuW7jjdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "oX-Qeh5jg-_A",
        "outputId": "07f2655a-2afd-47f8-d45d-1ce01d8fef91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jwj2eoo2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dimension: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220507_124912-jwj2eoo2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/anandh/CS6910_Assignment3_S2S/runs/jwj2eoo2\" target=\"_blank\">crimson-sweep-93</a></strong> to <a href=\"https://wandb.ai/anandh/CS6910_Assignment3_S2S\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/anandh/CS6910_Assignment3_S2S/sweeps/5y0u4iyv\" target=\"_blank\">https://wandb.ai/anandh/CS6910_Assignment3_S2S/sweeps/5y0u4iyv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1066/1066 [==============================] - 102s 89ms/step - loss: 0.8106 - accuracy: 0.7705 - _timestamp: 1651927860.0000 - _runtime: 108.0000\n",
            "Epoch 2/25\n",
            "1066/1066 [==============================] - 92s 87ms/step - loss: 0.6543 - accuracy: 0.8064 - _timestamp: 1651927952.0000 - _runtime: 200.0000\n",
            "Epoch 3/25\n",
            " 252/1066 [======>.......................] - ETA: 1:10 - loss: 0.6066 - accuracy: 0.8203"
          ]
        }
      ],
      "source": [
        "# run sweeps\n",
        "sweep_config = {\n",
        "    'method': 'bayes',  # grid, random\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'embedding_size': {\n",
        "            'values': [64,128,256]\n",
        "        },\n",
        "        'num_encoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'num_decoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'latent_dimension': {\n",
        "            'values': [64, 256, 512]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['RNN', 'GRU', 'LSTM']\n",
        "        },                             \n",
        "        'dropout': {\n",
        "            'values': [0.3,0.4,0.5,0.0,0.2]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [25,20,30]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "#sweep_id = wandb.sweep(sweep_config,entity=\"anandh\" ,project=\"CS6910_Assignment3_S2S\")\n",
        "# wandb.agent(sweep_id, fit, count=10)\n",
        "sweep_id=\"5y0u4iyv\"\n",
        "wandb.agent(sweep_id, fit, entity=\"anandh\", project = \"CS6910_Assignment3_S2S\", count = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGFpa91ehjkG"
      },
      "outputs": [],
      "source": [
        "#Test accuracy\n",
        "\n",
        "def test_accuracy(val_encoder_input_data, val_target_texts,n_decoder_layers,encoder_model,decoder_model,test_input_texts ,verbose=False):\n",
        "        n_correct = 0\n",
        "        n_total = 0\n",
        "        inputs=[]\n",
        "        outputs=[]\n",
        "        ground_truth=[]\n",
        "        for seq_index in range(len(val_encoder_input_data)):\n",
        "            # Take one sequence (part of the training set)\n",
        "            # for trying out decoding.\n",
        "            input_seq = val_encoder_input_data[seq_index: seq_index + 1]\n",
        "            # Generate empty target sequence of length 1.\n",
        "            # empty_seq = np.zeros((1, 1))\n",
        "            # # Populate the first character of target sequence with the start character.\n",
        "            # empty_seq[0, 0] = self.target_token_index[\"\\t\"]\n",
        "            decoded_sentence = decode_sequence(input_seq,n_decoder_layers,'GRU',encoder_model,decoder_model)\n",
        "\n",
        "            if decoded_sentence.strip() == val_target_texts[seq_index].strip():\n",
        "                n_correct += 1\n",
        "\n",
        "            n_total += 1\n",
        "\n",
        "            if verbose:\n",
        "                print('Prediction ', decoded_sentence.strip(), ',Ground Truth ', val_target_texts[seq_index].strip())\n",
        "            inputs.append(test_input_texts[seq_index])\n",
        "            outputs.append(decoded_sentence.strip())\n",
        "            ground_truth.append(val_target_texts[seq_index].strip())\n",
        "        df_train = pd.DataFrame({\"Input\": inputs, \"Ground Truth\" : ground_truth, \"Model output\":outputs})\n",
        "        #print(df_train)\n",
        "        df_train.to_csv('predictions_seq2seq2.csv', index=False)\n",
        "\n",
        "        return n_correct * 100.0 / n_total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy\n",
        "\n",
        "subset = 50\n",
        "test_accuracy = accuracy(test_encoder_input_data[0:subset], test_target_data[0:subset],n_decoder_layers,encoder_model,decoder_model) if subset>0 \\\n",
        "    else accuracy(test_encoder_input_data, test_target_data,n_decoder_layers,encoder_model,decoder_model)\n",
        "print('Validation accuracy: ', test_accuracy)"
      ],
      "metadata": {
        "id": "BOLRTB-2lC2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}