{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandhc6/Assignment-3/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhDxe46ViCCn"
      },
      "outputs": [],
      "source": [
        "# Required packages\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import backend\n",
        "from random import randrange \n",
        "from tensorflow import keras\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading dataset\n",
        "\n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf 'dakshina_dataset_v1.0.tar'"
      ],
      "metadata": {
        "id": "vKjM7uQ1hwl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6wy9xCNDc0u"
      },
      "outputs": [],
      "source": [
        "# embedding train data\n",
        "\n",
        "def embed_train_data(train_data_lines):\n",
        "\n",
        "    lenk = len(train_data_lines) - 1\n",
        "    train_input_data = []\n",
        "    train_target_data = []\n",
        "    input_data_characters = set()\n",
        "    target_data_characters = set()\n",
        "    \n",
        "    for line in train_data_lines[: lenk]:\n",
        "        target_data, input_data, _ = line.split(\"\\t\")\n",
        "\n",
        "        # We are using \"tab\" as the \"start sequence\" and \"\\n\" as \"end sequence\".\n",
        "        target_data = \"\\t\" + target_data + \"\\n\"\n",
        "        train_input_data.append(input_data)\n",
        "        train_target_data.append(target_data)\n",
        "\n",
        "        # Finding unique characters.\n",
        "        for ch in input_data:\n",
        "            if ch not in input_data_characters:\n",
        "                input_data_characters.add(ch)\n",
        "        for ch in target_data:\n",
        "            if ch not in target_data_characters:\n",
        "                target_data_characters.add(ch)\n",
        "\n",
        "    print(\"Number of samples:\", len(train_input_data))\n",
        "    # adding space \n",
        "    input_data_characters.add(\" \")\n",
        "    target_data_characters.add(\" \")\n",
        "\n",
        "    # sorting\n",
        "    input_data_characters = sorted(list(input_data_characters))\n",
        "    target_data_characters = sorted(list(target_data_characters))\n",
        "\n",
        "    # maximum length of the words\n",
        "    encoder_max_length = max([len(txt) for txt in train_input_data])\n",
        "    decoder_max_length = max([len(txt) for txt in train_target_data])\n",
        "\n",
        "    print(\"Max sequence length for inputs:\", encoder_max_length)\n",
        "    print(\"Max sequence length for outputs:\", decoder_max_length)\n",
        "\n",
        "    # number of input and target characters\n",
        "    num_encoder_tokens = len(input_data_characters)\n",
        "    num_decoder_tokens = len(target_data_characters)  \n",
        "    \n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "\n",
        "    # create an index\n",
        "    input_token_idx = dict([(char, i) for i, char in enumerate(input_data_characters)])\n",
        "    target_token_idx = dict([(char, i) for i, char in enumerate(target_data_characters)])\n",
        "   \n",
        "    # creating 0 array for encoder,decoder \n",
        "    encoder_input_data = np.zeros((len(train_input_data), encoder_max_length), dtype=\"float32\")\n",
        "\n",
        "    decoder_input_data = np.zeros((len(train_input_data), decoder_max_length), dtype=\"float32\")\n",
        "\n",
        "    decoder_target_data = np.zeros((len(train_input_data), decoder_max_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    # index of the character is encoded for all the sample whereas target data is one hot encoded.\n",
        "    for i, (input_data, target_data) in enumerate(zip(train_input_data, train_target_data)):\n",
        "        for t, char in enumerate(input_data):\n",
        "            encoder_input_data[i, t] = input_token_idx[char]\n",
        "        \n",
        "        encoder_input_data[i, t + 1:] = input_token_idx[\" \"]\n",
        "        \n",
        "        # decoder data\n",
        "        for t, char in enumerate(target_data):\n",
        "            # decoder_target_data is one timestep ahead of decoder_input_data\n",
        "            decoder_input_data[i, t] = target_token_idx[char]\n",
        "\n",
        "            if t > 0:\n",
        "                # excluding the start character since decoder target data is one timestep ahead.\n",
        "                decoder_target_data[i, t - 1, target_token_idx[char]] = 1.0\n",
        "        # append the remaining positions with empty space\n",
        "       \n",
        "        decoder_input_data[i, t + 1:] = target_token_idx[\" \"]\n",
        "        decoder_target_data[i, t:, target_token_idx[\" \"]] = 1.0\n",
        "\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data,num_encoder_tokens,num_decoder_tokens,input_token_idx,target_token_idx,encoder_max_length,decoder_max_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N80Xht1DZvW"
      },
      "outputs": [],
      "source": [
        "# embedding validation data\n",
        "\n",
        "def embed_val_data(val_data_lines,num_decoder_tokens,input_token_idx,target_token_idx):\n",
        "    val_input_data = []\n",
        "    val_target_data = []\n",
        "    lenk = len(val_data_lines) - 1\n",
        "\n",
        "    for line in val_data_lines[: lenk]:\n",
        "        target_data, input_data, _ = line.split(\"\\t\")\n",
        "        \n",
        "        # We use \"tab\" as the \"start sequence\" character and \"\\n\" as \"end sequence\" character.\n",
        "        target_data = \"\\t\" + target_data + \"\\n\"\n",
        "        val_input_data.append(input_data)\n",
        "        val_target_data.append(target_data)\n",
        "\n",
        "    val_encoder_max_length = max([len(txt) for txt in val_input_data])\n",
        "    val_decoder_max_length = max([len(txt) for txt in val_target_data])\n",
        "\n",
        "    val_encoder_input_data = np.zeros((len(val_input_data), val_encoder_max_length), dtype=\"float32\")\n",
        "    val_decoder_input_data = np.zeros((len(val_input_data), val_decoder_max_length), dtype=\"float32\")\n",
        "    val_decoder_target_data = np.zeros((len(val_input_data), val_decoder_max_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, (input_data, target_data) in enumerate(zip(val_input_data, val_target_data)):\n",
        "        for t, ch in enumerate(input_data):\n",
        "            val_encoder_input_data[i, t] = input_token_idx[ch]\n",
        "        val_encoder_input_data[i, t + 1:] = input_token_idx[\" \"]\n",
        "        \n",
        "        for t, ch in enumerate(target_data):\n",
        "            # decoder_target_data is one timestep ahead of decoder_input_data\n",
        "            val_decoder_input_data[i, t] = target_token_idx[ch]\n",
        "            if t > 0:\n",
        "                # excluding the start character since decoder target data is one timestep ahead.\n",
        "                val_decoder_target_data[i, t - 1, target_token_idx[ch]] = 1.0\n",
        "       \n",
        "        val_decoder_input_data[i, t + 1:] = target_token_idx[\" \"]\n",
        "        val_decoder_target_data[i, t:, target_token_idx[\" \"]] = 1.0\n",
        "\n",
        "    return val_encoder_input_data,val_decoder_input_data,val_decoder_target_data,target_token_idx,val_target_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SphcvHgrD6D2",
        "outputId": "b133ffc5-b2ee-4fc3-bd19-4546902933f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 68217\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n"
          ]
        }
      ],
      "source": [
        "# Embedding data\n",
        "encoder_input_data,decoder_input_data,decoder_target_data,num_encoder_tokens,num_decoder_tokens,input_token_idx,target_token_idx,encoder_max_length,decoder_max_length = embed_train_data(train_data_lines)\n",
        "\n",
        "val_encoder_input_data,val_decoder_input_data,val_decoder_target_data,target_token_idx,val_target_data = embed_val_data(val_data_lines,num_decoder_tokens,input_token_idx,target_token_idx)\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_idx.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_idx.items())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}