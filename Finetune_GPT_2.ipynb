{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetune GPT-2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandhc6/Assignment-3/blob/main/Finetune_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZMbGCw0Qxfg"
      },
      "source": [
        "# **Finetuning GPT2 using HuggingFace and Tensorflow**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzB7m0GI5fSt"
      },
      "source": [
        "Setup by cloning transformers repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rmSAUQVIUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e862d1c-44da-47b4-97a6-2216394fc86d"
      },
      "source": [
        "#Clone the transformers repo into the notebook\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 93572, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 93572 (delta 93), reused 101 (delta 58), pack-reused 93429\u001b[K\n",
            "Receiving objects: 100% (93572/93572), 85.83 MiB | 11.45 MiB/s, done.\n",
            "Resolving deltas: 100% (68602/68602), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350e452c-1383-4885-fb63-83c5e070dd05",
        "id": "SwmMUkDNsYj-"
      },
      "source": [
        "# Clone should now be in the machine\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5642d4eb-dc3f-469f-e1fc-44502c09aa3c",
        "id": "olQr383WsYj_"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 10 06:13:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLYIOlmsYj_"
      },
      "source": [
        "Changing directory location.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4016a8c8-2139-4f7c-83ca-d6d844d0fb09",
        "id": "W6YTY9yLsYj_"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers\")\n",
        "os.chdir(\"./examples/tensorflow/language-modeling\")\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md  requirements.txt  run_clm.py  run_mlm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254844b6-32e2-43f7-83f9-505bf7f78427",
        "id": "BW9HRnc7sYj_"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets>=1.8.0\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 93.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.3.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 75.7 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 84.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 81.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 98.6 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 88.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, sentencepiece, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 sentencepiece-0.1.96 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487789b9-0f6e-4627-e5fa-fca9affcd8fb",
        "id": "PqorlZEHsYj_"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md  requirements.txt  run_clm.py  run_mlm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e47ee41-551b-43b1-aebd-9a666271bdd2",
        "id": "fc9emArVsYj_"
      },
      "source": [
        "!pip install pyarrow --upgrade"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-8.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "Successfully installed pyarrow-8.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XifajmOPsYkA"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers/examples/\")\n",
        "os.chdir(\"./tensorflow/language-modeling\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d84f8b-1355-4fd9-a437-6fa81abf90fe",
        "id": "n6qaIJPrsYkA"
      },
      "source": [
        "#Installing latest transformer packages\n",
        "! pip install git+https://github.com/huggingface/transformers/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers/\n",
            "  Cloning https://github.com/huggingface/transformers/ to /tmp/pip-req-build-03bevd1u\n",
            "  Running command git clone -q https://github.com/huggingface/transformers/ /tmp/pip-req-build-03bevd1u\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 88.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=4115037 sha256=270d2306c6458abce7c7d649b6028ed20aba31bc78a608b6aa76026a4f97afc0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ke_4c17t/wheels/fb/1b/91/0fcf504c386d427d65bbaf663eadf8e18cbf9795394ed7050d\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18106f5-2e7b-4958-d04f-d55bde30cbf2",
        "id": "PeU58cBOsYkB"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esL59rNWsYkC"
      },
      "source": [
        "#Load Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/content/total_data (1).txt', 'r') as data:\n",
        "  dataset = [\"<|title|>\" + x.strip() for x in data.readlines()]\n",
        "\n",
        "train, eval = train_test_split(dataset, train_size=.9, random_state=2020)\n",
        "with open('/content/training.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(train))\n",
        "\n",
        "with open('/content/evaluation.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(eval))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwipFnaYsYkC"
      },
      "source": [
        "The script below will fine tune GPT2 on your text data that you setup above. This training step will take anywhre from tens of minutes to hours depending on how large your training set is, how many epochs you intend to train on, and if you are using colab or colab pro. We utilize mixed precision in this model to shave off some training time. For a large data set I was using for another experiment it saved us over 30 mins in training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e78bf8-45a3-4b84-de3a-2b2571e83917",
        "id": "KbsXXvUgsYkC"
      },
      "source": [
        "!python run_clm.py \\\n",
        "--model_name_or_path distilgpt2 \\\n",
        "--train_file \"/content/training.txt\" \\\n",
        "--do_train \\\n",
        "--validation_file \"/content/evaluation.txt\" \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 128 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 15 \\\n",
        "--fp16 \\\n",
        "--output_dir=\"/content/output_final_1\"\n",
        "#--model_type gpt2-medium \\"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\", line 22, in <module>\n",
            "    from tensorflow.python.data.util import nest\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/nest.py\", line 36, in <module>\n",
            "    from tensorflow.python.framework import sparse_tensor as _sparse_tensor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/sparse_tensor.py\", line 24, in <module>\n",
            "    from tensorflow.python.framework import constant_op\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 25, in <module>\n",
            "    from tensorflow.python.eager import execute\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 19, in <module>\n",
            "    from google.protobuf import text_format\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/protobuf/text_format.py\", line 54, in <module>\n",
            "    from google.protobuf.internal import type_checkers\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/protobuf/internal/type_checkers.py\", line 301, in <module>\n",
            "    0.0, float, numbers.Real),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/protobuf/internal/type_checkers.py\", line 149, in __init__\n",
            "    TypeChecker.__init__(self, *acceptable_types)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/protobuf/internal/type_checkers.py\", line 126, in __init__\n",
            "    def __init__(self, *acceptable_types):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_clm.py\", line 36, in <module>\n",
            "    import datasets\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/__init__.py\", line 37, in <module>\n",
            "    from .arrow_dataset import Dataset, concatenate_datasets\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 53, in <module>\n",
            "    from huggingface_hub import HfApi, HfFolder\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/huggingface_hub/__init__.py\", line 63, in <module>\n",
            "    from .keras_mixin import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/huggingface_hub/keras_mixin.py\", line 24, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 37, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 42, in <module>\n",
            "    from tensorflow.python import data\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python.data import experimental\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 95, in <module>\n",
            "    from tensorflow.python.data.experimental import service\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/service/__init__.py\", line 387, in <module>\n",
            "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 23, in <module>\n",
            "    from tensorflow.python.data.experimental.ops import compression_ops\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 16, in <module>\n",
            "    from tensorflow.python.data.util import structure\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 321, in __exit__\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jwzX9fbsYkC"
      },
      "source": [
        "# **Using the model**\n",
        "Next lets take our model we just trained and use it to generate some text! We will import the Tensorflow version of the gpt2 language model and set the from_pt flag to True. Then we load a pretrained tokenizer from huggingface. This may take some time to download the tokenizer data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVMrb5A5sYkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cafc565-6948-43a2-a7d4-67f9c748b992"
      },
      "source": [
        "# setup imports to use the model\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"/content/output_final_1\", from_pt=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/output_final_1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kUgS7Dm1sYkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564fe68a-ba29-486d-e1e7-b53d428d2851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n8HxsqdsYkD"
      },
      "source": [
        "Encoding sample text is now extremely simple using the pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1F0n1E5sYkD"
      },
      "source": [
        "input_ids = tokenizer.encode(\"I love Deep Learning\", return_tensors='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwXr0RzGsYkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd651a53-677f-4a85-d14e-33e45e2793ac"
      },
      "source": [
        "# the tf tensor object\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([   40,  1842, 10766, 18252], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVHwqJUTsYkD"
      },
      "source": [
        "Next we will use the model to generate the text from our input sample. The parameters I used are based on trail and error from playing around with the huggingface tutorial, https://huggingface.co/blog/how-to-generate, which really goes into great detail on how to go about finding the best parameters for generating text. As well they dive into really good information on what each parameter does and how they play into one another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s4RitnmsYkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ccdb0e-b231-4e8b-d9fd-777e8a9671f7"
      },
      "source": [
        "generated_text_samples = model.generate(\n",
        "    input_ids, \n",
        "    max_length=500,  \n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        ")\n",
        "beam_output = generated_text_samples\n",
        "for i, beam in enumerate(beam_output):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I love Deep Learning, can't you see?\n",
            "\n",
            "1: I love Deep Learning, but I need to talk again\n",
            "\n",
            "2: I love Deep Learning\n",
            "\n",
            "3: I love Deep Learning\n",
            "\n",
            "4: I love Deep Learning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPKHpTxHsYkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9376613a-b18c-42a3-e280-aa5dd5735dd2"
      },
      "source": [
        "#Print output for each sequence generated above\n",
        "beam_output = generated_text_samples\n",
        "for i, beam in enumerate(beam_output):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I love Deep Learning, can't you see?\n",
            "\n",
            "1: I love Deep Learning, but I need to talk again\n",
            "\n",
            "2: I love Deep Learning\n",
            "\n",
            "3: I love Deep Learning\n",
            "\n",
            "4: I love Deep Learning\n",
            "\n"
          ]
        }
      ]
    }
  ]
}